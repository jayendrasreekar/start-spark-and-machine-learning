{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ETL with Spark (Export / Transform / Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the libraries we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# https://search.maven.org/artifact/com.memsql/memsql-spark-connector_2.11\n",
    "# https://search.maven.org/artifact/com.microsoft.azure/azure-sqldb-spark\n",
    "\n",
    "args = ('--packages'\n",
    "  ' \"com.memsql:memsql-spark-connector_2.11:3.0.0-spark-2.4.4'\n",
    "  ',com.microsoft.azure:azure-sqldb-spark:1.0.2\"'\n",
    "  ' pyspark-shell')\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(appName=\"ETLApp\")\n",
    "# or\n",
    "# sc = SparkContext('local[*]')\n",
    "# or\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster(\"local\").setAppName(\"SparkApp\")\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the whole table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "server = \"sqlserver\"\n",
    "database = \"tpch\"\n",
    "table = \"dbo.line_item\"\n",
    "user = \"sa\"\n",
    "password = \"P@SSw0rd.\"\n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{server}:1433;database={database}\"\n",
    "connectionProperties = {\n",
    "  \"user\" : user,\n",
    "  \"password\": password,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "mssqldata = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we haven't loaded the data yet, only crafted the pipeline\n",
    "\n",
    "let's look at the data just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(mssqldata)\n",
    "mssqldata.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also run SQL queries\n",
    "\n",
    "parenthasis around query are required: https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases#push-down-a-query-to-the-database-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"sqlserver\"\n",
    "database = \"tpch\"\n",
    "table = \"dbo.line_item\"\n",
    "user = \"sa\"\n",
    "password = \"P@SSw0rd.\"\n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{server}:1433;database={database}\"\n",
    "connectionProperties = {\n",
    "  \"user\" : user,\n",
    "  \"password\": password,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "pushdown_query=\"(SELECT top 10 * FROM dbo.line_item) line_item\"\n",
    "mssqldata2 = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mssqldata2)\n",
    "mssqldata2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can filter in spark too, but now we've pulled back the whole table and then thrown data away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedcols = mssqldata.select(\"l_partkey\", \"l_quantity\", \"extended_price\").groupBy(\"l_partkey\").avg(\"extended_price\")\n",
    "display(selectedcols)\n",
    "selectedcols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to MemSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first select some data to make sure we can connect correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ddlEndpoint is MemSQL's word for server name\n",
    "memsqldata = spark.read.format(\"memsql\") \\\n",
    "    .option(\"ddlEndpoint\", \"memsql\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"\") \\\n",
    "    .option(\"database\", \"tpch\") \\\n",
    "    .load(\"line_item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(memsqldata)\n",
    "memsqldata.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's save all the sql server data into memsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mssqldata.write \\\n",
    "    .format(\"memsql\") \\\n",
    "    .option(\"ddlEndpoint\", \"memsql\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"\") \\\n",
    "    .option(\"database\", \"foo\") \\\n",
    "    .option(\"loadDataCompression\", \"LZ4\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"line_item\") # format: database.table or option(\"database\", \"...\n",
    "# creates table if not exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at data in MemSQL: http://localhost:8080\n",
    "```\n",
    "use tpch;\n",
    "select * from line_item;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
